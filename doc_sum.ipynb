{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d47d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is an example of text that needs to be tokenized.', 'The sent_tokenize function from the nltk library will split the text into sentences.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sahil\n",
      "[nltk_data]     Raj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Sentence segmentation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Dhawan scored 109 runs off 105 balls, and shared a 158-run stand with skipper Virat Kohli who scored 75. The match was interrupted by rain and inclement weather on more than one occasion which saw the target being reduced to 202 runs off 28 overs for South Africa. The Indian bowlers dominated South African batsmen in the initial stages.\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d4ec5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[None None None]]\n",
      "\n",
      " [[None None None]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[['This is an example of text that needs to be tokenized.', 0,\n",
       "         None]],\n",
       "\n",
       "       [['The sent_tokenize function from the nltk library will split the text into sentences.',\n",
       "         1, None]]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "emptyarray= np.empty((len(sentences),1,3),dtype=object)\n",
    "# print(emptyarray)\n",
    "for s in range(len(sentences)):\n",
    "    emptyarray[s][0][0] = sentences[s]\n",
    "    emptyarray[s][0][1] = s\n",
    "emptyarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5d4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8591924c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.0, 100.0]\n",
      "bitokens feature vector: [3, 6]\n"
     ]
    }
   ],
   "source": [
    "# Bi-grams\n",
    "\n",
    "bi_token=[]\n",
    "bi_token_length=[]\n",
    "tri_token_length=[]\n",
    "\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "#     print(sent_split)\n",
    "    bigrams_list = [bigram for bigram in nltk.bigrams(sent_split)]\n",
    "#     print(bigrams_list)\n",
    "    bi_token.append(bigrams_list)\n",
    "    bi_token_length.append(len(bi_token[u]))\n",
    "\n",
    "bi_tokens = [(int(o) / max(bi_token_length))*100 for o in bi_token_length]\n",
    "print(bi_tokens)\n",
    "print(\"bitokens feature vector:\",(bi_token_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1fc06e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.0, 100.0]\n",
      "tritokens feature vector: [2, 5]\n"
     ]
    }
   ],
   "source": [
    "# Tri-grams\n",
    "\n",
    "tri_token=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split2=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split3=[w for w in sent_split2 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    trigrams_list = [trigram for trigram in nltk.trigrams(sent_split3)]\n",
    "    tri_token.append(trigrams_list)\n",
    "    tri_token_length.append(len(tri_token[u]))\n",
    "tri_tokens = [(int(m) / max(tri_token_length))*100 for m in tri_token_length]\n",
    "print(tri_tokens)\n",
    "print(\"tritokens feature vector:\",tri_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "435ac972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence position: [0, 1]\n",
      "Total number of sentences: 2\n"
     ]
    }
   ],
   "source": [
    "# Sentence Position Feature\n",
    "\n",
    "import math\n",
    "def position(l):\n",
    "    return [index for index, value in enumerate(sentences)]\n",
    "\n",
    "sent_position= (position(sentences))\n",
    "print(\"sentence position:\",sent_position)\n",
    "num_sent=len(sent_position)\n",
    "print(\"Total number of sentences:\",num_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "408460ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence position feature vector: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "position = []\n",
    "position_rbm = []\n",
    "\n",
    "# sentence postion feature of first sentence\n",
    "sent_pos1_rbm = 1\n",
    "sent_pos1 = 100\n",
    "position.append(sent_pos1)\n",
    "position_rbm.append(sent_pos1_rbm)\n",
    "\n",
    "# for all sentences except first and last\n",
    "for x in range(1,num_sent-1):\n",
    "    s_p= ((num_sent-x)/num_sent)*100\n",
    "    position.append(s_p)\n",
    "    s_p_rbm = (num_sent-x)/num_sent\n",
    "    position_rbm.append(s_p_rbm)\n",
    "    \n",
    "# sentence postion feature of last sentence\n",
    "sent_pos2_rbm = 1\n",
    "sent_pos2 = 100\n",
    "position.append(sent_pos2)\n",
    "position_rbm.append(sent_pos2_rbm)\n",
    "\n",
    "print(\"Sentence position feature vector:\",position_rbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5331c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceVectors: [[2, 3, 0, 0, 1, 1, 1, 1, 8, 1, 3, 1, 1, 3, 6, 0, 2, 0, 1, 0, 10, 0, 3, 1, 2, 3], [3, 1, 1, 1, 1, 1, 1, 1, 10, 2, 8, 1, 5, 4, 11, 2, 0, 1, 1, 3, 12, 1, 4, 2, 1, 6]]\n"
     ]
    }
   ],
   "source": [
    "# Converting Sentences to Vectors\n",
    "\n",
    "def convertToVSM(sentences):\n",
    "    vocabulary = []\n",
    "    for sents in sentences:\n",
    "        vocabulary.extend(sents)\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    vectors = []\n",
    "    for sents in sentences:\n",
    "        vector = []\n",
    "        for tokenss in vocabulary:\n",
    "            vector.append(sents.count(tokenss))\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "VSM=convertToVSM(sentences)\n",
    "# print(sentences)\n",
    "print(\"SentenceVectors:\",VSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3710fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-ISF vector: [1.4, 1.86]\n",
      "Max TF-ISF: [1.86]\n",
      "Centroid: [3, 1, 1, 1, 1, 1, 1, 1, 10, 2, 8, 1, 5, 4, 11, 2, 0, 1, 1, 3, 12, 1, 4, 2, 1, 6]\n"
     ]
    }
   ],
   "source": [
    "# TF-ISF feature and Centroid Calculation\n",
    "\n",
    "sentencelength=len(sentences)\n",
    "def calcMeanTF_ISF(VSM, index):\n",
    "    vocab_len = len(VSM[index])\n",
    "    sentences_len = len(VSM)\n",
    "    count = 0\n",
    "    tfisf = 0\n",
    "    for i in range(vocab_len):\n",
    "        tf = VSM[index][i]\n",
    "        if(tf>0):\n",
    "            count += 1\n",
    "            sent_freq = 0\n",
    "            for j in range(sentences_len):\n",
    "                if(VSM[j][i]>0): sent_freq += 1\n",
    "            tfisf += (tf)*(1.0/sent_freq)\n",
    "    if(count > 0):\n",
    "        mean_tfisf = tfisf/count\n",
    "    else:\n",
    "        mean_tfisf = 0\n",
    "    return tf, (1.0/sent_freq), mean_tfisf\n",
    "tfvec=[]\n",
    "isfvec=[]\n",
    "tfisfvec=[]\n",
    "tfisfvec_rbm=[]\n",
    "for i in range(sentencelength):\n",
    "    x,y,z=calcMeanTF_ISF(VSM,i)\n",
    "    tfvec.append(x)\n",
    "    isfvec.append(y)\n",
    "    tfisfvec.append(z*100)\n",
    "    tfisfvec_rbm.append(z)\n",
    "print(\"TF-ISF vector:\",tfisfvec_rbm)\n",
    "maxtf_isf=max(tfisfvec_rbm)\n",
    "centroid=[]\n",
    "centroid.append(maxtf_isf)\n",
    "print(\"Max TF-ISF:\",centroid)\n",
    "centroid=(max(VSM))\n",
    "print(\"Centroid:\",centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99c225c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Vector: [0.9242780860054097, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity between Centroid and Sentences\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "cosine_similarity=[]\n",
    "cosine_similarity_rbm=[]\n",
    "for z in range(sentencelength):\n",
    "    cos_simi = ((dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))*100)\n",
    "    cosine_similarity.append(cos_simi)\n",
    "    cos_simi_rbm = (dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))\n",
    "    cosine_similarity_rbm.append(cos_simi_rbm)\n",
    "print(\"Cosine Similarity Vector:\",cosine_similarity_rbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf7c5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence length feature vector: [0.5714285714285714, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Sentence length feature\n",
    "\n",
    "sent_word=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    a=(len(sent_split))\n",
    "    sent_word.append(a)\n",
    "# print(sent_word)\n",
    "\n",
    "# LENGTH OF SENTENCE/ LONGEST SENTENCE\n",
    "longest_sent=max(sent_word)\n",
    "sent_length=[]\n",
    "sent_length_rbm=[]\n",
    "for x in sent_word:\n",
    "    sent_length.append((x/longest_sent)*100)\n",
    "    sent_length_rbm.append(x/longest_sent)\n",
    "#print(sent_length)\n",
    "\n",
    "print(\"Sentence length feature vector:\",sent_length_rbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "651fd8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric token feature vector: [0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Numeric token Feature\n",
    "\n",
    "import re\n",
    "num_word=[]\n",
    "numeric_token=[]\n",
    "numeric_token_rbm=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split4=sentences[u].split(\" \")\n",
    "    e=re.findall(\"\\d+\",sentences[u])\n",
    "    noofwords=(len(e))\n",
    "    num_word.append(noofwords)\n",
    "    numeric_token.append((num_word[u]/sent_word[u])*100)\n",
    "    numeric_token_rbm.append(num_word[u]/sent_word[u])\n",
    "print(\"Numeric token feature vector:\",numeric_token_rbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367d7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
