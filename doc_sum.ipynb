{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d47d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is an example of text that needs to be tokenized.', 'The sent_tokenize function from the nltk library will split the text into sentences.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sahil\n",
      "[nltk_data]     Raj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Sentence segmentation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"This is an example of text that needs to be tokenized. The sent_tokenize function from the nltk library will split the text into sentences.\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4ec5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[['This is an example of text that needs to be tokenized.', 0,\n",
       "         None]],\n",
       "\n",
       "       [['The sent_tokenize function from the nltk library will split the text into sentences.',\n",
       "         1, None]]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "emptyarray= np.empty((len(sentences),1,3),dtype=object)\n",
    "for s in range(len(sentences)):\n",
    "    emptyarray[s][0][0] = sentences[s]\n",
    "    emptyarray[s][0][1] = s\n",
    "emptyarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e5d4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8591924c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.0, 100.0]\n",
      "bitokens feature vector: [3, 6]\n"
     ]
    }
   ],
   "source": [
    "# Bi-grams\n",
    "\n",
    "bi_token=[]\n",
    "bi_token_length=[]\n",
    "tri_token_length=[]\n",
    "\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "#     print(sent_split)\n",
    "    bigrams_list = [bigram for bigram in nltk.bigrams(sent_split)]\n",
    "#     print(bigrams_list)\n",
    "    bi_token.append(bigrams_list)\n",
    "    bi_token_length.append(len(bi_token[u]))\n",
    "\n",
    "bi_tokens = [(int(o) / max(bi_token_length))*100 for o in bi_token_length]\n",
    "print(bi_tokens)\n",
    "print(\"bitokens feature vector:\",(bi_token_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1fc06e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.0, 100.0]\n",
      "tritokens feature vector: [2, 5]\n"
     ]
    }
   ],
   "source": [
    "# Tri-grams\n",
    "\n",
    "tri_token=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split2=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split3=[w for w in sent_split2 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    trigrams_list = [trigram for trigram in nltk.trigrams(sent_split3)]\n",
    "    tri_token.append(trigrams_list)\n",
    "    tri_token_length.append(len(tri_token[u]))\n",
    "tri_tokens = [(int(m) / max(tri_token_length))*100 for m in tri_token_length]\n",
    "print(tri_tokens)\n",
    "print(\"tritokens feature vector:\",tri_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "435ac972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence position: [0, 1]\n",
      "Total number of sentences: 2\n"
     ]
    }
   ],
   "source": [
    "# Sentence Position Feature\n",
    "\n",
    "import math\n",
    "def position(l):\n",
    "    return [index for index, value in enumerate(sentences)]\n",
    "\n",
    "sent_position= (position(sentences))\n",
    "print(\"sentence position:\",sent_position)\n",
    "num_sent=len(sent_position)\n",
    "print(\"Total number of sentences:\",num_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "408460ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence position feature vector: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "position = []\n",
    "position_rbm = []\n",
    "\n",
    "# sentence postion feature of first sentence\n",
    "sent_pos1_rbm = 1\n",
    "sent_pos1 = 100\n",
    "position.append(sent_pos1)\n",
    "position_rbm.append(sent_pos1_rbm)\n",
    "\n",
    "# for all sentences except first and last\n",
    "for x in range(1,num_sent-1):\n",
    "    s_p= ((num_sent-x)/num_sent)*100\n",
    "    position.append(s_p)\n",
    "    s_p_rbm = (num_sent-x)/num_sent\n",
    "    position_rbm.append(s_p_rbm)\n",
    "    \n",
    "# sentence postion feature of last sentence\n",
    "sent_pos2_rbm = 1\n",
    "sent_pos2 = 100\n",
    "position.append(sent_pos2)\n",
    "position_rbm.append(sent_pos2_rbm)\n",
    "\n",
    "print(\"Sentence position feature vector:\",position_rbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5331c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is an example of text that needs to be tokenized.', 'The sent_tokenize function from the nltk library will split the text into sentences.']\n",
      "SentenceVectors: [[2, 1, 6, 8, 1, 3, 1, 0, 2, 0, 0, 1, 10, 3, 3, 1, 3, 3, 1, 1, 2, 1, 0, 0, 1, 0], [3, 1, 11, 10, 2, 1, 1, 1, 0, 2, 3, 1, 12, 8, 4, 5, 6, 4, 2, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Converting Sentences to Vectors\n",
    "\n",
    "def convertToVSM(sentences):\n",
    "    vocabulary = []\n",
    "    for sents in sentences:\n",
    "        vocabulary.extend(sents)\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    vectors = []\n",
    "    for sents in sentences:\n",
    "        vector = []\n",
    "        for tokenss in vocabulary:\n",
    "            vector.append(sents.count(tokenss))\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "VSM=convertToVSM(sentences)\n",
    "print(sentences)\n",
    "print(\"SentenceVectors:\",VSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3710fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
